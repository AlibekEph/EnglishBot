import logging
import os
import configparser
import requests
import time
from typing import Dict, Any
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import asyncio
from telegram.error import TimedOut, NetworkError, RetryAfter
import backoff  # –î–æ–±–∞–≤–∏–º –≤ requirements_bot.txt
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('bot.log')
    ]
)

logger = logging.getLogger(__name__)
llm_logger = logging.getLogger('llm')
telegram_logger = logging.getLogger('telegram')

class LLMProvider:
    def __init__(self, config):
        self.config = config
        self.local_model_url = "http://llm-server:8000"
        self.min_confidence = float(config['LLM']['MIN_CONFIDENCE'])
        self.use_openai = config.getboolean('OpenAI', 'USE_OPENAI', fallback=False)
        
        if self.use_openai:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OpenAI
            self.openai_api_key = config['OpenAI']['API_KEY']
            self.openai_model = config['OpenAI']['MODEL']
            self.openai_max_tokens = int(config['OpenAI'].get('MAX_TOKENS', 150))
            self.openai_temperature = float(config['OpenAI'].get('TEMPERATURE', 0.7))
            llm_logger.info("Using OpenAI API with model: %s", self.openai_model)
        else:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.default_model = config['LLM']['DEFAULT_MODEL']
            self.available_local_models = config['LocalLLM']['AVAILABLE_MODELS'].split(',')
            
            # –ï—Å–ª–∏ DEFAULT_MODEL = 'local', –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            if self.default_model == 'local':
                self.default_model = self.available_local_models[0]
                llm_logger.info(f"Using first available local model: {self.default_model}")
            elif self.default_model not in self.available_local_models:
                llm_logger.warning(
                    f"Default model {self.default_model} not available locally, "
                    f"using first available model: {self.available_local_models[0]}"
                )
                self.default_model = self.available_local_models[0]
            
            llm_logger.info("Using local model server at: %s with model: %s", 
                          self.local_model_url, self.default_model)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    async def check_text(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            if not self.use_openai:
                llm_logger.info(f"Using local model {self.default_model} for text check")
                return await self._check_with_local_model(text)
            
            # –ï—Å–ª–∏ USE_OPENAI = true, –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI API
            llm_logger.info(f"Using OpenAI API with model {self.openai_model} for text check")
            return await self._check_with_openai(text)
                
        except Exception as e:
            llm_logger.error(f"Error checking text: {str(e)}", exc_info=True)
            raise

    async def _check_with_local_model(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if self.use_openai:
            raise ValueError("Cannot use local model when USE_OPENAI is true")
            
        try:
            llm_logger.info(f"Checking text with local model: {self.default_model}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
            if self.default_model not in self.available_local_models:
                raise ValueError(f"Model {self.default_model} not found in available models: {self.available_local_models}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
            prompt = (
                "Check the following text for grammar, spelling, and punctuation errors. "
                "If you find any errors, provide the corrected text and explain the corrections. "
                "If the text is correct, respond with 'No errors found.' "
                "Format your response as JSON with the following structure: "
                '{"has_errors": boolean, "corrected_text": string, "explanation": string, "confidence": float (0-1)}\n\n'
                f"Text to check: {text}"
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É API
            request_data = {
                "model": self.default_model,
                "text": prompt,
                "temperature": 0.7,
                "max_tokens": 256
            }
            llm_logger.debug(f"Sending request to local API: {self.local_model_url}/generate with data: {request_data}")
            response = self.session.post(
                f"{self.local_model_url}/generate",
                json=request_data,
                timeout=(30, 60)
            )
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['text'].strip()
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                parsed_result = json.loads(content)
                llm_logger.info(f"Local model response: {parsed_result}")
                return parsed_result
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                llm_logger.warning(f"Could not parse local model response as JSON: {content}")
                return {
                    "has_errors": "error" in content.lower(),
                    "corrected_text": text,
                    "explanation": content,
                    "confidence": 0.8 if "no error" in content.lower() else 0.6
                }
                
        except requests.exceptions.RequestException as e:
            llm_logger.error(f"Error calling local model API: {str(e)}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {str(e)}")

    async def _check_with_openai(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é OpenAI API"""
        if not self.use_openai:
            raise ValueError("Cannot use OpenAI API when USE_OPENAI is false")
            
        try:
            llm_logger.info(f"Checking text with OpenAI API using model: {self.openai_model}")
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful English grammar checker. Check the text for grammar, spelling, and punctuation errors. If you find any errors, provide the corrected text and explain the corrections. If the text is correct, respond with 'No errors found.' Format your response as JSON with the following structure: {\"has_errors\": boolean, \"corrected_text\": string, \"explanation\": string, \"confidence\": float (0-1)}"
                },
                {
                    "role": "user",
                    "content": text
                }
            ]
            
            data = {
                "model": self.openai_model,
                "messages": messages,
                "temperature": self.openai_temperature,
                "max_tokens": self.openai_max_tokens
            }
            
            response = self.session.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=(30, 60)
            )
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                parsed_result = json.loads(content)
                llm_logger.info(f"OpenAI API response: {parsed_result}")
                return parsed_result
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                llm_logger.warning(f"Could not parse OpenAI response as JSON: {content}")
                return {
                    "has_errors": "error" in content.lower(),
                    "corrected_text": text,
                    "explanation": content,
                    "confidence": 0.8 if "no error" in content.lower() else 0.6
                }
                
        except requests.exceptions.RequestException as e:
            llm_logger.error(f"Error calling OpenAI API: {str(e)}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI API: {str(e)}")

class EnglishGrammarBot:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self.config = configparser.ConfigParser()
            config_path = os.path.join(os.path.dirname(__file__), 'settings.ini')
            llm_logger.info(f"Loading configuration from: {config_path}")
            
            if not self.config.read(config_path):
                raise ValueError(f"Could not read configuration file: {config_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–µ–∫—Ü–∏–π
            required_sections = ['Telegram', 'LLM', 'OpenAI', 'LocalLLM', 'Bot']
            for section in required_sections:
                if section not in self.config:
                    raise ValueError(f"Missing required section in settings.ini: {section}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM
            self.llm_provider = LLMProvider(self.config)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ—Ç–∞
            self.token = self.config['Telegram']['BOT_TOKEN']
            self.min_confidence = float(self.config['Bot']['MIN_CONFIDENCE'])
            self.max_message_length = int(self.config['Bot']['MAX_MESSAGE_LENGTH'])
            
            llm_logger.info("Bot initialized successfully with settings from settings.ini")
            llm_logger.info(f"USE_OPENAI setting: {self.llm_provider.use_openai}")
            
        except Exception as e:
            llm_logger.error(f"Error initializing bot: {str(e)}", exc_info=True)
            raise

    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
        user = update.effective_user
        telegram_logger.info(f"User {user.id} ({user.username}) started the bot")
        await update.message.reply_text(
            "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏. "
            "–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∏ —è –ø—Ä–æ–≤–µ—Ä—é –µ–≥–æ –Ω–∞ –æ—à–∏–±–∫–∏."
        )

    async def help(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
        user = update.effective_user
        telegram_logger.info(f"User {user.id} ({user.username}) requested help")
        help_text = """
        –Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏.
        
        –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∏ —è:
        1. –ü—Ä–æ–≤–µ—Ä—é –µ–≥–æ –Ω–∞ –æ—à–∏–±–∫–∏
        2. –ï—Å–ª–∏ –Ω–∞–π–¥—É –æ—à–∏–±–∫–∏, –ø—Ä–µ–¥–ª–æ–∂—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        3. –û–±—ä—è—Å–Ω—é –æ—à–∏–±–∫–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        
        –ö–æ–º–∞–Ω–¥—ã:
        /start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º
        /help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
        await update.message.reply_text(help_text)

    @backoff.on_exception(
        backoff.expo,
        (TimedOut, NetworkError, RetryAfter),
        max_tries=5,
        max_time=300
    )
    async def initialize_bot(self, application: Application):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        try:
            logger.info("Initializing bot...")
            await application.initialize()
            logger.info("Bot initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing bot: {str(e)}", exc_info=True)
            raise

    async def process_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–æ–±—â–µ–Ω–∏–∏
            message = update.message
            if not message or not message.text:
                return

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Ç–µ
            chat = message.chat
            chat_type = chat.type
            chat_id = chat.id
            user_id = message.from_user.id
            username = message.from_user.username

            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–æ–±—â–µ–Ω–∏–∏
            telegram_logger.info(
                f"Received message in {chat_type} chat {chat_id} "
                f"from user {user_id} (@{username}): {message.text[:50]}..."
            )

            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã
            if message.text.startswith('/'):
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è
            if len(message.text) > self.max_message_length:
                await message.reply_text(
                    f"–°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {self.max_message_length} —Å–∏–º–≤–æ–ª–æ–≤."
                )
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            try:
                result = await self.llm_provider.check_text(message.text)
                
                if result.get('has_errors', False):
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
                    corrected_text = result.get('corrected_text', '')
                    explanation = result.get('explanation', '')
                    confidence = result.get('confidence', 0.0)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                    if confidence >= self.min_confidence:
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
                        response = f"üí° –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{corrected_text}\n\n"
                        if explanation:
                            response += f"üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:\n{explanation}"
                        
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                        await message.reply_text(response)
                        llm_logger.info(
                            f"Sent correction in {chat_type} chat {chat_id} "
                            f"with confidence {confidence:.2f}"
                        )
                    else:
                        llm_logger.debug(
                            f"Confidence {confidence:.2f} below threshold {self.min_confidence}, "
                            f"ignoring correction"
                        )
                else:
                    llm_logger.debug("No errors found in the text")
                    
            except Exception as e:
                llm_logger.error(f"Error checking text: {str(e)}", exc_info=True)
                await message.reply_text(
                    "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–µ–∫—Å—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                )

        except Exception as e:
            llm_logger.error(f"Error processing message: {str(e)}", exc_info=True)
            if message:
                await message.reply_text(
                    "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                )

    def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        try:
            llm_logger.info("Starting bot...")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
            application = Application.builder().token(self.config['Telegram']['BOT_TOKEN']).build()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
            application.add_handler(MessageHandler(
                filters.TEXT & ~filters.COMMAND,  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫—Ä–æ–º–µ –∫–æ–º–∞–Ω–¥
                self.process_message
            ))
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
            llm_logger.info("Bot started successfully")
            application.run_polling(
                allowed_updates=Update.ALL_TYPES,  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
                drop_pending_updates=True,        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–∞–∫–æ–ø–∏–≤—à–∏–µ—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
                connect_timeout=30,               # –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
                read_timeout=30,                  # –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è
                write_timeout=30,                 # –¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Å–∏
                pool_timeout=30                   # –¢–∞–π–º–∞—É—Ç –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            )
            
        except Exception as e:
            llm_logger.error(f"Error running bot: {str(e)}", exc_info=True)
            raise

if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    bot_instance = EnglishGrammarBot()
    bot_instance.run() 