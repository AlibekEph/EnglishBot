import logging
import os
import configparser
import requests
import time
from typing import Dict, Any
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import asyncio
from telegram.error import TimedOut, NetworkError, RetryAfter
import backoff  # –î–æ–±–∞–≤–∏–º –≤ requirements_bot.txt
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('bot.log')
    ]
)

logger = logging.getLogger(__name__)
llm_logger = logging.getLogger('llm')
telegram_logger = logging.getLogger('telegram')

class LLMProvider:
    def __init__(self, config):
        self.config = config
        self.local_model_url = "http://llm-server:8000"
        self.min_confidence = float(config['LLM']['MIN_CONFIDENCE'])
        self.use_openai = config.getboolean('OpenAI', 'USE_OPENAI', fallback=False)
        
        if self.use_openai:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OpenAI
            self.openai_api_key = config['OpenAI']['API_KEY']
            self.openai_model = config['OpenAI']['MODEL']
            self.openai_max_tokens = int(config['OpenAI'].get('MAX_TOKENS', 150))
            self.openai_temperature = float(config['OpenAI'].get('TEMPERATURE', 0.7))
            llm_logger.info("Using OpenAI API with model: %s", self.openai_model)
        else:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.default_model = config['LLM']['DEFAULT_MODEL']
            self.available_local_models = config['LocalLLM']['AVAILABLE_MODELS'].split(',')
            
            # –ï—Å–ª–∏ DEFAULT_MODEL = 'local', –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            if self.default_model == 'local':
                self.default_model = self.available_local_models[0]
                llm_logger.info(f"Using first available local model: {self.default_model}")
            elif self.default_model not in self.available_local_models:
                llm_logger.warning(
                    f"Default model {self.default_model} not available locally, "
                    f"using first available model: {self.available_local_models[0]}"
                )
                self.default_model = self.available_local_models[0]
            
            llm_logger.info("Using local model server at: %s with model: %s", 
                          self.local_model_url, self.default_model)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    async def _check_with_local_model(self, text: str, model: str) -> dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if model not in self.available_local_models:
            raise ValueError(f"Model {model} not found in available models: {self.available_local_models}")
            
        max_retries = 3
        retry_delay = 5  # —Å–µ–∫—É–Ω–¥
        
        for attempt in range(max_retries):
            try:
                llm_logger.info(f"Checking text with local model {model} (attempt {attempt + 1}/{max_retries})")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                if len(text) > 200:
                    text = text[:200] + "..."
                    llm_logger.warning(f"Text truncated to 200 characters")
                
                response = self.session.post(
                    f"{self.local_model_url}/generate",
                    json={
                        "text": text,
                        "model": model,
                        "max_tokens": min(int(self.config['LLM']['MAX_TOKENS']), 128),
                        "temperature": float(self.config['LLM']['TEMPERATURE'])
                    },
                    timeout=(30, 60)
                )
                response.raise_for_status()
                result = response.json()
                llm_logger.info(f"Local model response: {result}")
                return result
                
            except requests.exceptions.RequestException as e:
                llm_logger.error(f"Error calling local model (attempt {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1)
                    llm_logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {str(e)}")

    async def _check_with_openai(self, text: str) -> dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é OpenAI API"""
        try:
            llm_logger.info("Checking text with OpenAI API")
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful English grammar checker. Check the text for grammar, spelling, and punctuation errors. If you find any errors, provide the corrected text and explain the corrections. If the text is correct, respond with 'No errors found.' Format your response as JSON with the following structure: {\"has_errors\": boolean, \"corrected_text\": string, \"explanation\": string, \"confidence\": float (0-1)}"
                },
                {
                    "role": "user",
                    "content": text
                }
            ]
            
            data = {
                "model": self.openai_model,
                "messages": messages,
                "temperature": self.openai_temperature,
                "max_tokens": self.openai_max_tokens
            }
            
            response = self.session.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=(30, 60)
            )
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                parsed_result = json.loads(content)
                llm_logger.info(f"OpenAI API response: {parsed_result}")
                return parsed_result
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                llm_logger.warning(f"Could not parse OpenAI response as JSON: {content}")
                return {
                    "has_errors": "error" in content.lower(),
                    "corrected_text": text,
                    "explanation": content,
                    "confidence": 0.8 if "no error" in content.lower() else 0.6
                }
                
        except requests.exceptions.RequestException as e:
            llm_logger.error(f"Error calling OpenAI API: {str(e)}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI API: {str(e)}")

    async def check_text(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"""
        try:
            if self.use_openai:
                llm_logger.info("Using OpenAI API for text check")
                return await self._check_with_openai(text)
            else:
                llm_logger.info(f"Using local model {self.default_model} for text check")
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if len(text) > 200:
                    sentences = text.split('.')
                    results = []
                    for sentence in sentences:
                        if sentence.strip():
                            result = await self._check_with_local_model(sentence.strip(), self.default_model)
                            results.append(result)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if results:
                        has_errors = any(r.get('has_errors', False) for r in results)
                        best_result = max(results, key=lambda x: x.get('confidence', 0))
                        return {
                            'has_errors': has_errors,
                            'corrected_text': best_result.get('corrected_text', text),
                            'explanation': best_result.get('explanation', ''),
                            'confidence': best_result.get('confidence', 0)
                        }
                
                return await self._check_with_local_model(text, self.default_model)
                
        except Exception as e:
            llm_logger.error(f"Error checking text: {str(e)}", exc_info=True)
            raise

class EnglishGrammarBot:
    def __init__(self):
        self.config = configparser.ConfigParser()
        self.config.read('settings.ini')
        self.llm = LLMProvider(self.config)
        self.min_confidence = float(self.config['Bot']['MIN_CONFIDENCE'])
        logger.info("Bot initialized with settings from settings.ini")

    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
        user = update.effective_user
        telegram_logger.info(f"User {user.id} ({user.username}) started the bot")
        await update.message.reply_text(
            "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏. "
            "–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∏ —è –ø—Ä–æ–≤–µ—Ä—é –µ–≥–æ –Ω–∞ –æ—à–∏–±–∫–∏."
        )

    async def help(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
        user = update.effective_user
        telegram_logger.info(f"User {user.id} ({user.username}) requested help")
        help_text = """
        –Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏.
        
        –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∏ —è:
        1. –ü—Ä–æ–≤–µ—Ä—é –µ–≥–æ –Ω–∞ –æ—à–∏–±–∫–∏
        2. –ï—Å–ª–∏ –Ω–∞–π–¥—É –æ—à–∏–±–∫–∏, –ø—Ä–µ–¥–ª–æ–∂—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        3. –û–±—ä—è—Å–Ω—é –æ—à–∏–±–∫–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        
        –ö–æ–º–∞–Ω–¥—ã:
        /start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º
        /help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
        await update.message.reply_text(help_text)

    @backoff.on_exception(
        backoff.expo,
        (TimedOut, NetworkError, RetryAfter),
        max_tries=5,
        max_time=300
    )
    async def initialize_bot(self, application: Application):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        try:
            logger.info("Initializing bot...")
            await application.initialize()
            logger.info("Bot initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing bot: {str(e)}", exc_info=True)
            raise

    def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        logger.info("Starting bot...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
        application = (
            Application.builder()
            .token(self.config['Telegram']['BOT_TOKEN'])
            .connect_timeout(30.0)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            .read_timeout(30.0)     # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è
            .write_timeout(30.0)    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Å–∏
            .pool_timeout(30.0)     # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –ø—É–ª–∞
            .build()
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        application.add_handler(CommandHandler("start", self.start))
        application.add_handler(CommandHandler("help", self.help))
        application.add_handler(MessageHandler(
            filters.TEXT | filters.ChatType.GROUPS | filters.ChatType.SUPERGROUP,
            self.process_message
        ))

        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
        try:
            logger.info("Bot is ready to receive messages")
            application.run_polling(
                allowed_updates=Update.ALL_TYPES,
                drop_pending_updates=True,  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                pool_timeout=30.0,          # –¢–∞–π–º–∞—É—Ç –¥–ª—è –ø—É–ª–∞
                read_timeout=30.0,          # –¢–∞–π–º–∞—É—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è
                write_timeout=30.0,         # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏
                connect_timeout=30.0        # –¢–∞–π–º–∞—É—Ç –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            )
        except Exception as e:
            logger.error(f"Error running bot: {str(e)}", exc_info=True)
            # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            time.sleep(5)
            # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
            self.run()

    @backoff.on_exception(
        backoff.expo,
        (TimedOut, NetworkError, RetryAfter),
        max_tries=3,
        max_time=60
    )
    async def process_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if not update.message or not update.message.text:
            return

        user = update.effective_user
        chat = update.effective_chat
        text = update.message.text

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Ç–µ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
        chat_type = "private" if chat.type == "private" else f"{chat.type} ({chat.title})"
        telegram_logger.info(
            f"Message in {chat_type} from {user.id} ({user.username}): {text}"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–æ–π
        if text.startswith('/'):
            telegram_logger.debug(f"Ignoring command message: {text}")
            return

        if len(text) > int(self.config['Bot']['MAX_MESSAGE_LENGTH']):
            telegram_logger.warning(
                f"Message too long from user {user.id} in {chat_type}: {len(text)} chars"
            )
            if chat.type == "private":  # –û—Ç–≤–µ—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
                await update.message.reply_text(
                    f"–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ. "
                    f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {self.config['Bot']['MAX_MESSAGE_LENGTH']} —Å–∏–º–≤–æ–ª–æ–≤."
                )
            return

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é LLM
            result = await self.llm.check_text(text)
            
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if result['confidence'] < self.min_confidence:
                telegram_logger.warning(
                    f"Low confidence result ({result['confidence']}) for user {user.id} in {chat_type}"
                )
                return

            if result['has_errors']:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
                response = (
                    f"üîç –ù–∞–π–¥–µ–Ω—ã –æ—à–∏–±–∫–∏:\n\n"
                    f"üìù –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:\n{result['corrected_text']}\n\n"
                    f"üìö –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:\n{result['explanation']}"
                )
                telegram_logger.info(
                    f"Sending correction to user {user.id} in {chat_type}"
                )
                
                # –í –≥—Ä—É–ø–ø–æ–≤—ã—Ö —á–∞—Ç–∞—Ö –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ
                if chat.type != "private":
                    await update.message.reply_text(
                        response,
                        reply_to_message_id=update.message.message_id
                    )
                else:
                    await update.message.reply_text(response)
            else:
                telegram_logger.info(
                    f"No errors found for user {user.id} in {chat_type}"
                )
            
        except Exception as e:
            logger.error(
                f"Error processing message from user {user.id} in {chat_type}: {str(e)}",
                exc_info=True
            )
            if chat.type == "private":  # –û—Ç–≤–µ—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
                await update.message.reply_text(
                    "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                )

if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    bot_instance = EnglishGrammarBot()
    bot_instance.run() 